Ce chapitre propose un digest de la reproductibilité, et en particulier de la reproductibilité computationnelle selon le point de vue de l'histoire des sciences

Intro
-----

L'expression "crise de la reproductibilité" en français possède [son article Wikipédia](https://fr.wikipedia.org/w/index.php?title=Crise_de_la_reproductibilit%C3%A9&action=history) depuis fin 2016 et l'équivalent en anglais (reproducibility crisis ou replication crisis) [depuis début 2015](https://en.wikipedia.org/w/index.php?title=Replication_crisis&dir=prev&action=history). Une rapide recherche du terme "reproducibility crisis" dans les "[google trends](https://trends.google.com/trends/explore?date=all&q=%2Fm%2F012mc030)" montre que cette expression se popularise depuis la première moitié des années 2010.
Le journal "Nature" en a fait ces dernières années un de ses sujets éditoriaux récurrents. En particulier, [une enquête sous forme de questionnaire](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970) de plusieurs centaines de scientifiques de tous domaines est publiée en 2016 et est depuis reprise par tous les articles évoquant le sujet : Oui, il existe selon la majorité des scientifiques interrogés une incapacité à reproduire les expériences scientifiques publiées, et oui, il s'agit selon eux d'une crise, sous entendant non seulement que l'affaire est grave, mais aussi qu'elle est nouvelle (Baker, 2016).

La reproductibilité est souvent réclamée comme la moindre des choses en science, et pourtant, en forçant un peu le trait, on pourrait avancer que quasiment rien n'est jamais reproduit : personne ne veut essayer, et quand quelqu'un essaye, ça ne marche pas. Ce qui est nouveau, c'est que cela semble rédhibitoire dans le cadre de bonnes pratiques scientifiques, au nom de la crédibilité de la science, et [des initiatives](https://rescience.github.io/) pour y remédier commencent à fleurir.

Dans ce chapitre, nous allons essayer de nous intéresser à la crise de la reproductibilité du point de vue de l'histoire (et aussi philosophie et sociologie) des sciences.  Il est frappant de constater que dans la tentative de survol bibliométrique des occurences du terme par Barba (Barba, 2018), seulement une occurence concerne une philosophe des sciences (Cartwright, 1991), et encore s'agit il d'une réponse à un chapitre d'un collègue, à l'intérieur d'un livre dédié à l'économie. Il est probable que l'utilisation du Web of Science biaise les résultats en défaveur des revues scientifiques spécialisées en sciences humaines (sans compter que ces communautés publient souvent dans des livres), mais il est quand même frappant que la communauté scientifique censée la mieux connaître le sujet soit ou bien muette, ou bien inaudible.
Nous alons donc faire un état de l'art probalement pas exhaustif mais qui résumera quelques prises de position sur ce thème et nous terminerons en particulier sur la question qui nous intéresse le plus ici, qui est celle de la place de la reproductibilité computationnelle dans la crise de la reproductibilité.

La reproductibilité est une notion complexe
-------------------------------------------

### TL;DR
~La reproductibilité est une question complexe : La diversité de termes (replicability? repeatability?, checking? robustness?) et la polysémie la caractérisent. Par qui (soi même, un collègue, un concurrent, un reviewer, une instance de vérification?) ? Pour quoi (pour valider?, pour contredire? pour interpréter?) Comment ? (même instrumentation ? même protocole ? même conclusion par d'autres moyens ?) qu'est ce qui est "pareil" ? (strictes mesures, patterns, généralisations ?) quand a t on besoin d'être "pareil" ? (pour démontrer, pour infirmer ou contredire, pour généraliser?). D'une manière générale, la littérature en histoire des sciences montre que si la reproductibilité conduit à plus de fiabilité en science, c'en est un moyen parmi d'autres, pas toujours suffisant, pas toujours nécessaire.~

Il ya tout d'abord une complexité sémantique. Non seulement de nombreux termes (en anglais particulièrement : replicability? repeatability?, checking? robustness?) mais aussi de nombreux sens différents associés à chacun de ces termes : Le sens exact de la reproductibilité est rarement reproductible. La raison de cette absence de consensus est probablement à chercher dans la diversité des communautés scientifiques (différentes cultures épistémiques, voir chapitre 3) qui s'emparent de cette notion. Une typologie des différents termes et de leurs emplois a déjà été esquissée (Baker, 2016b), mais l'analyse fine des généalogies (d'où viennent les différences et à quelles cultures correspondent elles?) serait un sujet de recherche intéressant en soi. Dans la suite de ce texte, on essaye de rester agnostique, tout en essayant de pointer la diversité. 

D'une manière générale, la littérature en histoire des sciences montre que si la reproductibilité conduit à plus de fiabilité en science, c'en est un moyen parmi d'autres, pas toujours suffisant, pas toujours nécessaire. La compilation historique de Steinle (Steinle, 2016) propose une variété de situations.

La reproductibilité est aussi complexe parce que, bien que simple à imaginer en apparence, le concept recouvre tout un tas de pratiques qui posent des questions : 

* Reproductibilité par qui ? soi même? un collègue? un concurrent? un reviewer? une instance de vérification?  
* Reproductibilité pour quoi : pour valider ?, pour contredire ? pour interpréter ? 
* Reproductibilité comment ? avec la même instrumentation ? le même protocole ? la même conclusion par d'autres moyens ?
* De fait, qu'est ce qui est "pareil" ?  de strictes mesures ? des patterns concordants de résultats ? des conclusions généralisatrices ?
* Et enfin,  quand a t on besoin d'être "pareil" ? pour vérifier ? pour démontrer ? pour infirmer ou contredire ? pour généraliser ?

Par ailleurs, la "reproduction" doit elle être hypothétique ou effective ? Il est notoire que les scientifiques n'ont (en général) aucune motivation à reproduire les expériences des autres : Puisque la récompense de l'activité de recherche est la publication et que la valeur de la publication réside dans l'originalité, la question reste souvent hypothétique, et les rares cas de tentatives de reproduction se font lors de controverses. Paradoxalement, malgré le flou qui l'entoure et la faible activité de reproduction en pratique, la reproductibilité est souvent citée comme une des activités de base de la bonne pratique scientifique, voire un "gold standard" de la science. Elle est par exemple pregnante dans la critique de l'activité de publication et en particulier du protocole de peer reviewing. La nécessité de la reproductibilité est ainsi souvent brandie comme un principe moral indiscutable, et les conditions de la réalisation de ce principe sont souvent hypothétiques. La critique de l'activité de reviewing, par exemple, existe de plus en plus et des propositions sont envisagées, mais cette remise en cause, parfois appliquée dans des revues scientifiques d'avant garde, ne pèse pas lourd face à l'immobilisme des revues les plus prestigieuses, celles qui pèsent le plus.

### vertus épistémiques : Schikore
Tout d'abord, la reproduction d'expériences peut avoir des **vertus épistemiques** différentes selon les contextes. Dans le cas d'études des effets de morsures de viperes par les scientifiques de l'Academia dei Cimento dans la Toscane du 16e siècle, l'historienne Jutta Schikore compare l'utilisation differente de la repetition des experiences par Redi et son disciple Fontana (Schikore, 2011). Si le premier se vante de répéter des centaines de fois ses expériences de morsures de vipères sur des grenouilles, c'est pour d'une part gérer l'incertitude générées par des grenouilles ou des vipères differentes, et d'autre part pour disqualifier les resultats de ses concurrents. Fontana, plus tard, essaie de comprendre la variabilité des résultats en isolant les cas qui ne "collent pas" et en les interprétant. Ce faisant, il en déduit une théorie du fonctionnement du poison. Schikore montre ainsi que la reproductibilité peut servir différentes fonctions épistémiques dans l'activité scientifique.

### La reproductibilité en histoire des sciences : Joule et les brasseries
En histoire des sciences, la tentative de reproduction d'expériences du passé est aussi une méthode pour essayer de révéler le contexte de ces expériences, au delà de la simple publication. Par exemple : l'historien Otto Sibum s'est interessé aux expériences de Joule sur la conversion de chaleur en travail (Sibum, 1995). Les résultats de Joule ne sont pas quantitativement reproductibles (la mesure de l'augmentation de température de l'eau sur laquelle on fait tomber un poids est très sensible aux conditions atmosphériques dans le laboratoire) et rien n'indique ni dans les publications, ni dans les cahiers de laboratoire ni dans la correpsondance privée de Joule comment lever l'indétermination sur cette variabilité. Sibum est arrivé à la conclusion que la connexion de Joule avec le monde des brasseurs de bière (dont l'industrialisation demandait un savoir faire de régulation de la température pendant la fermentation) était l'explication d'un savoir tacite qui lui permettait de gérer cette variabilité de température dans ces expériences.  La tentative de reproduction a permis de préciser la notion de savoir tacite **(tacit knowledge)**: ce que l'expérimentateur ne sait pas ou ne peut pas expliciter dans la réussite de son expérience, non pas seulement par négligence, mais aussi parce que certains savoirs ne sont par essence pas explicitables. En ce sens, la reproductibilité est un idéal inatteignable.

### conviction : Leviathan & the air pump
Selon les situations, le fait de reproduire englobe celui de convaincre autrui ou pas, ce qui change beaucoup de choses à la fois à l'exigence de ce qui est accepté comme identique dans l'activité de reproduction mais aussi des techniques utilisées pour gagner l'adhésion.
L'avènement de la pompe a air au 17e siècle est un exemple canonique. La crédibilité des expériences sur le vide de Otto von Guericke etait basée sur le spectacle. L'exceptionnalité de ses démonstrations publiques et l'appareillage complexe et unique de la pompe a vide rendait la reproduction inenvisageable, si ce n'est pour Guericke lui meme (et la garantie du succès de ses spectacles).
Les expériences sur le vide de Robert Boyle, basées sur le même principe expérimental, mais sur un appareillage différent, avaient besoin d'autre chose pour gagner en crédibilité. Le succès de ses experiences dependait de manière critique du savoir faire experimental de Hooke. Pour emporter l'adhesion sur le résultat de ses expériences, Boyle a eu recours à des témoins bien choisis (les gentlemen a la base de la fondation de la Royal Society, dignes de confiance puisque aristocrates). Au dela, la description la plus précise possible par écrit des expériences et des appareillages dans un compte rendu certifié par des gentlemen (et qui est l'ancêtre de la publication telle qu'on la connait aujourd'hui) consiste en ce que les historiens Shapin et Schaeffer (Shapin et Schaffer, 198) ont appelé un témoin virtuel (**virtual witness),** consigné par écrit. Il faut noter que personne, malgré les tantatives de Huyghens en France notamment, n'a réussi à reproduire une pompe à air fonctionnelle à l'aide de ces publications : Cette reproduction dépendait de trop de savoirs tacites. Le but de cette "technique litteraire" **(litterary technology)** (qui est à l'origine de la publication telle qu'on la connaît aujourd'hui) n'était effectivement pas (et n'est toujours pas) la reproductibilité mais la légitimité.

### controverses : Collins
Ce concept de *tacit knowledge*, décrit initialement par Polanyi a été développé et catégorisé par le sociologue des sciences Harry Collins. Collins est la principale référence quand il s'agit de théoriser la reproductibilité. Ses quelques études de cas (Le laser TEA, le facteur Q du saphir, les essais de mise en évidence des ondes gravitationnelles sur 40 ans) montrent l'influence de ces savoirs tacites dans les difficultés de reproduction.
L'école de la SSK (pour Sociology of Scientific Knowledge) s'est particuilèrement intéressée aux controverses scientifiques, parce qu'elles permettent d'en savoir plus sur ce qui se passe vraiment dans la science en train de se faire **(knowledge in the making)** par rapport à une situation sans problème. Etudier les controverses, c'est augmenter les chances de comprendres les formes de savoir tacite qui conditionnent la reproduction, qui n'apparaissent pas dans les publications, et qui surgissent au grand jour parce que des chercheurs contestent.
Dans le cas des essais expérimenatux de mise en évidence des ondes gravitationnelles (prévues par la théorie de la relativité), Collins a mis en évidence que les dispoitifs expérimentaux (ou plus exactement, les dispositifs de traitement du rapport signal/bruit produits par l'expérience) posent toute une série de problèmes (pointés par les équipes essayant de reproduire les résultats) qui empêchent le consensus. La controverse se clôt finalement sans qu'aucun parti ne puisse convaincre l'autre (Collins, 1985). Dans la formule "**experimenter's regress**", Collins pointe une indétermination qu'il considère irréductible. "*To know whether an experiment has been well conducted, one needs to know whether it gives rise to the correct outcome. But to know what the correct outcome is, one needs to do a well-conducted experiment. But to know whether the experiment has been well conducted ...ad infinitum. the experimenters’ regress shows that experiment alone cannot force a scientist to accept a view that they are determined to resist." *(Collins, 2016)*. *La "sociologie de la calibration" de Collins met l'accent sur la nécessité que les instruments et les protocoles expérimenatux gagnent en crédibilité pour que des conclusions basées sur des résultats expérimenataux soient acceptés par la communauté de scientifiques. Dans le cas des ondes gravitationnelles, l'affirmation de leur détecton a d'abord été rejetée dans les années 70 pour être finalement acceptée 40 ans plus tard. Shapin and Schaeffer ont repris cet exemple contemporain dans leur description des "litterary techniques" de Boyle pour ses pompes à air dans sa quête de légitimité.

Les catégories de Leonelli
--------------------------

### TL;DR
~La reproductibilité est donc multiiforme et dépendante du contexte mais elle est aussi différente selon les domaines scientifiques. Différente sur comment elle est perçue et est mise en place mais aussi différente sur son importance et sa normativité.~

Sabina Leonelli est une philosophe des sciences, elle s'intéresse à la "data-centric biology" (selon sa propre expression, c'est à dire à l'activité scientifique dans les sciences de la vie à l'ère des big data). Elle étudie ce qu'elle appelle les voyages des données. Les données, jamais "brutes", contiennent en elles toutes les théories, conditions, protocoles, biais, cultures qui ont servi à leur production et leur réutilisation se fait toujours dans d'autres conditions, par des chercheurs appartenant à d'autres cultures. Pour ce qui est de la reproductibilité, dans la même veine consitant à tenir compte des différentes cultures épistémiques de différents domaines scientifiques, Leonelli propose six catégories d'activités scientifiques pour lesquelles "reproductibilité" n'a pas forcément le même sens ni la même importance. (Leonelli 2018)

### 1 Computational Reproducibility
La reproductibilité computationnelle est la première envisagée par Leonelli parce qu'elle lui semble la plus simple à coller à une définition qui ne pose pas de problème. *A research project is computationally reproducible if a second investigator [..] can recreate the final reported results of the project, including key quantitative findings, tables, and figures, given only a set of files and written instructions.* On y trouve une vision de ce qu'est le computationnel restreint au  traitement de données, et la question statistique y est implicitement agrégée. (Voir le chapitre 5 "reproductibilité computationnelle" en quoi cela pose problème). Pour Leonelli, il s'agit du seul domaine où une reproductibilité "absolue" est à la fois envisageable et souhaitable.  

### 2 Direct Experimental Reproducibility: Standardised Experiments
La deuxième catégorie concerne la reproductibilité expérimentale que Leonelli nomme "directe". Elle concerne les expériences qui sont les plus facilement maîtrisables (elle cite les **essais cliniques en médecine** ou la **physique des particules**) et ou, par conséquent, la reproductibilité est un canon de l'activité scientifique : elle est désirée et essentielle.  Dans cette catégorie, contrairement (selon elle) à la première, *The circumstances of data production are, by contrast, a primary concern for experimentalists. Ces domaines sont caractérisés par un *gros contrôle sur les conditions, une attente de reproductibilité sur les patterns plutôt que sur les données exactes sortantes, et typiquement sur l'utilisation de la statistique pour trancher quant à ces patterns.

### 3 Scoping, Indirect and Hypothetical Reproducibility: Semi-Standardised Experiments
Dans sa troisème catégorie, *methods,  set-up  and  materials  used  have  been construed with ingenuity in order to yield very specific outcomes, and yet some significant parts of  the  set-up  necessarily  elude  the  controls  set  up  by experimenters. *Cela concerne par exemple la recherche sur les **organismes modèles** (rats de labo), la **psychologie sociale**, ou la **neuroscience** : toutes ces activités scientifiques ont en commun d'être impossibles à standardiser complètement. Les conclusions intéressantes proviennent justement de ce qui est "non-standardised" dans ces expériences, comme par exemple la répétition chez Fontana pour l'étude de la variabilité dans l'effet des morsures de vipères , alors que la répétition chez Redi n'a d'autre but que de gérer l'aléatoire. (Schikore 2011). Dans cette catégorie un peu fourre-tout, Leonelli propose que la reproductibilité la plus significative se trouve dans la *convergence across multiple lines of evidence,even when they are produced in different ways,is a mark of reliable research, *ce que les historiens des sciences ont appelé la triangulation ou la robustesse (Cartwright, 1991) : Arriver à des conclusions cohérentes entre elles à partir d'expériences qui n'ont rien à voir entre elles.

### 4 Reproducible Expertise: Non-Standard Experiments and Research on Rare Materials
La catégorie suivante s'intéresse à l'exceptionnalité : *cases  where experimenters  are  studying  new  objects  or  phenomena  (new  organisms  for instance)  and/or  employing  newly  devised,  unique  instruments  that are precisely tailored to the inquiry at hand. *Dans ces situations, Le fait d'être significatif est moins lié au contrôle des conditions expérimentales qu'à l'expertise de la gestion des conditions exceptionnlles : f*ocus less on controls and  more  on  developing  robust  ways  of  evaluating  the  effects  of  their interventions  and  the  relation  between  those  effects  and  the  experimental circumstances  at  the  time  in  which  data  were  collected. *Ce sont les sciences du rare qui sont ici concernées : en **archéologie**, la répétiton est tout simplement impossible et sans objet.  *uniqueness and irreproducibility of the materials is arguably what  makes  the  resulting  data particularly useful  as  evidence. *Dans cette catégorie, la vertu épistémique se trouve dans l'expertise :*  reproducible expertise [...] as the expectation that any skilled  experimenter  working  with  the  same  methods  and  the  same  type  of materials at that particular time and place would produce similar results.*

### 5 Reproducible Observation: Non-experimental case description
Les deux dernières catégories concernent les sciences de l'observation : *surveys,  descriptions  and  case  reports documenting unique  circumstances. L'expertise y est encore une fois clé pour l' "observation reproductible".  Reproducibility  of observation [is] the  expectation  that  any  skilled  researcher  placed  in  the same  time  and  place  would  pick  out,  if  not the  same data,  at  least similar patterns. *Leonelli cite la **sociologie** mais aussi la **radiologie** :* structured  interviewing,  where  researchers devise  a  relatively rigid  framing  for  their interactions  with  informants; and  diagnosis based on radiographies, resonance  scans and  other  medical  imaging  techniques.*

### 6 Irreproducible Research: Participant Observation
Enfin la dernière catégorie traite des activités scientifiques où *the idea of reproducibility has been rejected in favor of an embrace of the subjectivity and unavoidable context-dependence of research outcomes . *En **anthropologie**, la reproductibilité n'a pas de sens : *[Anthropologists]* *cannot rely on reproducibility as an epistemic criterion for data quality and validity. They therefore devote considerable care to documenting data production processes and strategizing about data preservation and dissemination. *Là où la reproductibilité n'a pas de sens, les communautés scientifiques font reposer leur crédibilité sur d'autres vertus épistémiques. La réflexivité, par exemple, (dont de nombreuses sciences pourraient s'inspirer). *Ethnographic work in anthropology, for instance, has developed methods to account for the fact that data are likely to change depending on time, place, subjects as well as researchers’ moods, experiences and interests. Key among such methods is the principle of reflexivity, which requires researchers to give as comprehensive a view of their personal circumstances*

Généralisation abusive
----------------------

### TL;DR
~La conclusion de cette typologie en six catégories est que l'exigence de reproductibilité (en tant que moyen d'obtenir la fiabilité) pose problème (et encore plus si elle est définie dans un sens étroit, correspondant à un seul domaine scientifique) pour la vitalité de champs scientifiques différents pour lesquels cette exigence peut être sans objet, voire contre-productive.~


### Ghettoïsation
Certains vont même plus loin et voient dans l'exigence de reproductibilité "one size fits all" une tentative de ghettoïsation des sciences qui ne correspondraient pas à ce standard trop facilement accepté comme universel (Penders et al. 2019). En s'appuyant sur les catégories de Leonelli et en reprenant ses inquiétudes quant à la généralisation abusive, Penders et al. voient dans la focalisation sur la reproductibilité en tant que critère obligatoire, une déligitimation des sciences humaines en général.
Le point le plus important ici est la disqualification d'une entreprise de définition de la reproductibilité qui se voudrait universelle, un peu comme celle qu'on trouve dans les analyses de méta-science, comme par exemple Goodman et al. 2016 qui proposent (dans le but de sortir de la polysémie) trois reproductibilités différentes, inspirées de leur expériences en recherche biomédicale mais qu'ils considèrent sans autre forme de procès comme applicables à tous les domaines scientifiques.

### Raw data
Problématique dans ce genre de littérature est aussi l'idée de "raw data" : Les données ne sont jamais brutes : elles portent en elles les théories / méthodes / biais / cultures de ceux (humains, instruments et protocoles) qui les ont imaginées / conçues [/](file:///) définies / produites / traitées / échangées ET ce qu'elles portent est redéfini à chaque nouveau contexte, nouvelle expérience. Une vision superficielle de ce qu'est une donnée conduit à une vision superficielle de la reproductibilité

### Norton
Comme l'énonce le philosophe des sciences John Norton : "*A failure of replication may not impugn a credible experimental result; and a successful replication can fail to vindicate an incredible experimental result. Rather, employing a material approach to inductive inference, the evidential import of successful replication of an experiment is determined by the prevailing background facts. Commonly, these background facts do support successful replication as a good evidential guide and this has fostered the illusion of a deeper, exceptionless principle.*" Ces "backgroud facts" sont spécifiques à chaque domaine scientifique : Les questions de reproductibilité ne se posent pas de la même manière selon les situations expérimentales : Cherche-ton à détecter le signal des ondes gravitationnlles dans le bruit des vibrations de quelques mm d'amplitude sur des bras métalliques de plusieurs km de long ? Cherche-t-on à rendre des populations de souris les plus significatives possibles mais suivent elles le même régime alimentaire que dans l'animalerie d'un autre laboratoire ? Il n'y a absolument rien d'universel là dedans, et chaque "culture épistémique" construit ses propres critères de légitimité, à l'intérieur desquels la reproductibilité peut trouver une place plus ou moins importante, et surtout, s'y exprime de différentes manières. De fait, la mention de "la méthode scientifique", comme si elle était unique, en tant que source de la reproductibilité "gold standard" est contradictoire avec cette diversité.  

La crise
--------

### TL;DR
~La narration de la reproductibilité en tant que crise pose question : Pourquoi est on en crise maintenant (depuis les années 2010?) et ce dans des domaines distincts (psychologie, épidémiologie, sciences computationnelles...) Quel est le lien avec la narration de la science ouverte ? de la crise de la publication (l'ouverture des données en est une des clés) ? Quelles sont les enjeux ? Comment et pourquoi ces éléments de discours sont ils repris/amorcés/amplifiés par des institutions (sociétés savantes, institutions nationales...) ? Quelle vision normative de ce que devrait être la bonne science cela traduit il? En quoi cette narration est elle liée à une crise de confiance (des citoyens, des institutions) envers la science et comment est elle gérée (par les institutions) ?~

### La crise dans les médias
A première vue, les signaux d'alerte envoyés et la façon de les lancer laisse penser qu'il existe un lien, ou du moins une concomittance avec le mouvement de l'Open Access, la rebellion contre les oligopoles des éditeurs scientifiques et leur tendance à rendre la littérature scientifique inaccessible au commun des mortels (voire au commun des chercheurs) : Un des chevaux de bataille de ce mouvement est la revendication de plus de transparence dans les sciences. Par extension, le mouvement de l'Open Science revendique que les expériences scientifiques puissent être reproductibles, dans le cadre de cette exigence de transparence.  La reproductibilité est revendiquée comme le "gold standard", l'étalon qui permet la confiance dans l'activité scientifique, confiance de la part de la communauté des chercheurs eux-mêmes mais aussi pour les institutions scientifiques de financement, et les citoyens. Le lien entre publication, transparence et reproductibilité est particulièrement pregnant dans la critique du peer-reviewing qui accompagne le mouvement de l'Open Access.

Une compréhension fine du phénomène mériterait qu'une étude se penche sur les publications s'emparant du sujet, à la fois dans les journaux scientifiques, dans la presse, dans les journaux à l'interface de ces deux sphères (comme Nature) mais aussi dans les communiqués d'institutions (sociétés savantes, agences de financement...). Une telle étude permettrait aussi de tracer des généalogies plus spécifiques, en particulier selon les domaines scientifiques, qui ne vivent pas cette crise tous de la même manière.

Enfin, la narration de la crise trouve ses sceptiques (Fanelli, 2018), en particulier dans la remise en cause de l'étendue des résultats irreproductibles, et surtout de leur augmentation récente. La science évolue, et avec elle ses propres critères de fiabilité, particulièrement en ce qui concerne la *significance* en statistiques.

### La crise selon Nelson
Nicole Nelson (Nelson, 2019) s'intéresse aux différentes narrations de la crise, en particulier dans le monde des essais cliniques. D'une analyse rapide (qui demanderait à être affinée) de la crise telle qu'elle est narrée, on peut distinguer trois dynamiques principales de la médiatisation de la crise.

#### En psychologie
D'un côté, en psychologie, la reproductibilité d'études scientifiques est souvent contestée, en particulier parce que ces études apparaissent parfois dans la presse grand public : elles sont médiatiques, donc exposées. Un exemple célèbre de tapage méditique autour de cette question est l'expérience ["feeling the future"](https://en.wikipedia.org/wiki/Daryl_Bem#.22Feeling_the_Future.22_controversy) en 2011. D'autre part, la psychologie est un domaine scientifique souvent sur la défensive, constamment sommé de justifier sa scientificité (voir par exemple la contestation des résultats des expériences de Milgram ou de la prison de Stanford). En 2011, est créé le Reproducibility Project, puis en 2013, le Center for Open Science, opération de reproduction d'expériences en psychologie impliquant toute une communauté scientifque. De nombreuses publications existent sur ce sujet dans les journaux de ce domaine, résumées par le livre-manifeste "The seven deadly sins of psychology: a manifesto for reforming the culture of scientific practice". La crise de la reproductibilité, en psychologie, correspond à une introspection de l'ensemble de ce champ scientifique afin de définir collectivement des bonnes pratiques scientifiques, comme la [preregistartion of studies as a good practice to improve reproducibility.](https://www.sciencemag.org/news/2019/05/solution-psychology-s-reproducibility-problem-just-failed-its-first-test) Ulrike Feest, philosophe des sciences sociales, propose que le champ scientifiqe redéfinisse ses pratiques en tant qu'exploration plutôt que de vaines tentatives de "reproduction" (Feest, 2016) .

#### Essais cliniques
L'autre domaine concerne les essais cliniques en médecine. Nicole Nelson en propose une généalogie. La médiatisation, en 2012, d'une publication affirmant avoir tenté de reproduire plusieurs dizaines d'essais cliniques de traitements contre le cancer pour un taux déchec proche de 90% provoque un tollé. La curiosité de cette publication que retient Nelson est que ses auteurs sont affiliés à une entreprise biomédicale privée. Il est notoire que les chercheurs n'ont a priori aucun intérêt (à l'exception de cas de compétitions ou disputes autour d'une controversev précise) à perdre leur temps à essayer de reproduire les résultats des autres puisque ça ne leur rapporte rien en termes d'originalité ni de publications. Dans le cas de chercheurs du privé, c'est encore plus étonnant puisque ça ne rapporte non plus rien financièrement.
L'historicisation des essais cliniques en médecine permet d'en comprendre les raisons. L' "evidence based medicine" est une politique (en particulier etats-unienne) apparue à la fin du XXe siècle fondée sur un espoir de rationalisation du processus de décision dans la pratique médicale. Les méta-analyses des essais cliniques ont connu  à cette occasion une nouvelle mise en lumière comparative (en particulier grâce aux graphiques en forêt). Une conséquence en a été le doute grandissant sur la validité des études cliniques devant la visualisation de résultats parfois apparemment contradictoires. Au début des années 2000, la contestation a porté sur les biais liés au fiancement privé des recherches, avec en point d'orgue le livre "The Truth About the Drug Companies: How They Deceive Us and What to Do" provenant de la communauté académique médicale. La mise en évidence récente de problèmes de reproductibilité par des études venant du privé suggère que ce manue de reproductibilité est lié à autre chose que l'influence d'intérêts financiers. Nelson le voit donc comme une sorte de contre-attaque de l'industrie pharmaceutique pour sortir du rôle de gros méchant dans lequel elle est cantonnée. 

#### La méta-science
L'autre angle de contestation est porté par les statisticiens. La publication de 2005 de Ioannidis "Why Most Published Research Findings Are False" (au titre particulièrement nuancé) est de loin la plus citée. Au point que depuis, un champ scientifique appelé "méta-science", censé analyser les problèmes de reproductibilité en science, se concentre exclusivement sur les problèmes posés par la statistique et analyse la question exclusivement sous l'angle des bonnes pratiques statistiques.
Il est de fait frappant que la quasi unanimité de la médiatisation de la crise concerne de fait le traitement statistique des résultats des expériences, et ce dans les deux domaines évoqués ici et dans la plupart des autres. C'est la troisième dynamique en question.

La reproductibilité computationnelle
------------------------------------

### TL;DR
~Comment situer la reproductibilité computationnelle dans ce panorama ? En quoi est elle moins visible ? En quoi elle est différente ou semblable ? Est elle en période de crise ?~

### Expérimental - Statistique - Computationnel
la reproductibilité computationnelle est ce qu'il nous intéresse ici. D'une part, la narration de la crise (telle que cette dernière apparait dans les médias a tendance à l'invisibiliser sous le flot de la crise statistique (voir le paragraphe 4). Si on distingue trois domaines différents dans laquelle la reproductibilité s'exprime (bien que les trois soient entremêlés) expérimental / statistique / computationnel, alors le statistique est la vedette de la crise. D'ailleurs, dans sa catégorisation, Leonelli a tendance à 1) confondre computationnel et statistique , 2) réduire le computationnel au traitement de données. 
Le statistique a tendance à être plus impliqué dans les domaines les plus médiatiques  (essais cliniques et psychologie). Le statistique est plus utilisé par des non-experts et il est plus facile à dramatiser, donc à médiatiser. L'apparition médiatique d'experts en "méta-science" comme Ioannidis, surfant sur cette vague de la crise de la reproductibilité en atteste.
La reproductibilité expérimentale est historiquement apparue en premier (voir chapitre 1). Les différents aspects qui la caractérisent dépendent de chaque domaine scientifique. Il est donc difficile d'en esquisser des principes généraux. Le philosophe John Norton argumente même qu'il n'existe pas de théorie générale du raisonnement par induction (Norton, 2010), et que chaque domaine scientifique possède ses propres critères méthodologiques, en particulier sur la question de la reproductibilité. La sociologue Knorr-Cetina montre en observant des physiciens des particules et des imunologistes que différents domaines scientifiques possèdent différentes "**cultures épistémiques**", battant en brèche l'idée qu'il existerait UNE méthode scientifique (Knorr-Cetina, 1999). C'est que les philosophes des sciences nomment "the disunity of science". Dans le même ordre d'idée, Collins, en invoquant le "experimenter's regress" énonce que la question de la reproductibilité doit d'abord résoudre le problème du consensus de ce que constitue le même "espace expérimental" qui va définir sur ce quoi un groupe de chercheurs peut se mettre d'accord quant à la validité des expériences. A l'intérieur de cet espace, un consensus sur la reproductibilité peut émerger, mais il ne serra jamais universel. 
Dans "How experiments end", Galison oppose deux catégories de scientifiques dans le même domaine de la physique des particules : ceux qui basent leur confiance dans les trajectoires effectivement observées dans les chambres à bulle et ceux qui ont plus confiance dans le traitement statistique Monte Carlo et la répétition des calculs ("anything can happen once"). On est là en présence d'une cohabitation de légitimités expérimentale et statistique (voire computationnelle) avec des stratégies différentes de définition de ce qui est fiable à l'intérieur d'un même champ scientifique (Galison, 1987).

### Généalogie
Pourtant, la reproductibilité comme norme, comme exigence, est souvent rhétoriquement liée à l'expérimentation, au discours de l'expérimentation. Les techniques de Boyle (voir chapitre 1) pour convaincre et acquérir une légitimité sont à l'origine de la fondation de la Royal Society et du concept de publication, et sont devenues l'archétype de la reproductibilité universelle telle qu'elle est fantasmée. C'est peut-être cette légitimation de l'expérience qui est à la base du discours des scientifiques sur le fait que la reproductibilité doit être le "golden standard" en science. Du coup, la reproductibilité computationnelle ne vient, chronologiquement, qu'après.

Et on peut même se poser la question de savoir si une part de la "crise de la reproductibilité" n'est pas liée à la place importante que les outils computationnels et statistiques ont pris dans les sciences contemporaines. N'y aurait-il pas un soupçon de non-reproductibilité de l'expérience à cause des traitements statistiques et computationnels appliqués aux "données" expérimentales, dans la même veine que la citation apocryphe de Churchill *"je ne crois aux statistiques que j'ai moi même falsifiées"*. Les controverses des ondes gravitationnlles dans les années 70 montrent déjà un déplacement du terrain de la reproduction du dispositif expérimental lui même vers son traitement statistique. La controverse s'est terminé en 2015 quand les traitements statistiques ont enfin fait consensus, et une des conclusions de Collins est que la validité statistique est grandement dépendante du domaine à laquelle on l'applique (Collins, 2016).

Si la reproductibilité statistique est grandement discutée (voir chapitre 4), il existe aussi une part de (problème de) reproductibilité qui est computationnelle sans pour autant être statistique, et elle a tendance à être parfois invisibilisée par ou confondue avec le statistique. Que ça soit en traitement de données, en modélisation, en informatique ou même dans quasiment tous les appareils électroniques de l'instrumentation, la computation intervient  dans des recoins de la science sans même que le scientifique s'en rende parfois compte.

D'ailleurs, le calcul computationnel est souvent réduit, y compris par ses acteurs mêmes, à l'activité de traitement de données par le calcul, ce qui favorise la confusion entre computationnel et statistique. De fait, le computationnel est vu comme "ce qui traite les données" : *Computer scientist Jon Claerbout coined the term and associated it with a software platform and set of procedures that permit the reader of a paper to see the entire processing trail from the raw data and code to figures and tables (Goodman 2016). *Pourtant, même s'ils sont interpénétrés, le statistique et le computationnel ne posent pas les mêmes problèmes de reproductibilité.

En poussant cette idée, il y aussi l'évolution de l'expérimentation du type "*single experimenter who builds his own apparatus and who knows its foibles and idiosyncrasies*" vers le "*research team with a marked division of labour and a hierarchical management structure*" (Bloor 1991 reviewant Galison) et j'ajouterais : l'instrument avec ses théories embarquées (Reinhardt, citant Bachelard) est aussi un objet pour lequel la confiance peut être basée sur des considérations commerciales (la garantie d'un instrument est fourni par le constructeur qui est une entreprise) et qui contient de plus en plus de computationnel (en particulier des logiciels de traitement du signal qui sont propriétaires puisque faisant partie de l'instrument industriel). Les spectroscopies sont particulièrement dans ce cas, et la RMN est l'exemple le plus cité (Reinhardt, 2006)

### Caractéristiques de la reproductibilité computationnelle
La reproductibilité computationnelle souffre elle d'un déficit de reconnaissance "grand public" : beaucoup en ont une image superficielle d'infaillibilité "2+2=4 est tout le temps vrai" et pourtant elle a eu aussi son moment de "crise grand pubic" avec le climategate suivi du Science Code Manifesto en 2011. A cette occasion, la révélation d'emails de chercheurs en science du climat avaient semé la panique dans la communauté dont la crédibilité est un enjeu hautement politique. Le point le plus controversé de ce climategate était l'utilisation de *tricks* dans la programmation des modèles (en pratique, les emails en question mentionnaient des commentaires de lignes de codes incluant des subroutines appelées "tricks"), ce qui a abouti à une prise de conscience des problèmes posés par la programmation scientifique, et à l'élaboration du Science Code Manifesto en 2011 (Bailey, 2011).
Parmi les caractéristiques de reproductibilité computationnelle évoquées dans ce booksprint, la plupart concerne le software. Comme la curation de données, le travail de programmation (et non seulement de programmation mais aussi de compilation, distribution, politique de licence...) par un chercheur n'est pas récompensé par la publication (sauf s'il s'agit de son activité de recherche elle même) (Hocquet Wieber 2017). De fait, une grosse partie de l'activité computationnelle est réalisée par des scientifiques dont ce n'est pas le métier : ni dans le coding, ni dans le management, ni dans la diffusion et le licensing. C'est la différence entre la computing science (qui produit et publie des programmes) et la computational (qui utilise des programmes dans son activité). Par ailleurs, l'industrie du software en général est elle même un domaine "en crise" depuis les années 60 (Ensmenger, 2011) : Contrairement au hardware qui est de plus en plus performant (voir la loi de Moore), le software est toujours en retard, toujours plus cher que prévu, à l'interopérabilité toujours plus compliquée, et à la stabilité et la cohérence jamais achevées : La "software crisis" est un art de vivre.
Même si la définition et l'organisation de bonnes pratiques sont des préoccupations des chercheurs (Rougier, 2018) et que ses bonnes pratiques sont souvent inspirées par les "libertés fondamentales du logiciel libre" établisssant en cela une connexion directe entre principes du logiciel libre et open science, Le logiciel scientifique possède une existence en pratique parfois sujette a des tensions entre normes académiques et normes commerciales, liées entre autres à cette absence de récompense : commercialisation de packages (éventuellement encouragées par les politiques de "technology transfer" des universités) (Hocquet Wieber 2017), ou inversement, énorme investissement en énergie et temps pour produire un logiciel libre, incapacité à imaginer un "business model" qui satisfasse à des éxigences épistémiques de trnasparence, protection du code par peur de la compétition, mais aussi par souci de stabilité du logiciel, licences variables selon les utilisateurs académiques ou industriels, bricolage de paramètres dans les modèles en fonction des utilisateurs (Wieber Hocquet, 2018). 
